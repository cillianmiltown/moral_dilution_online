---
title             : "Pilot Study 1"
shorttitle        : "Moral Dilution"
author:
  - name          : "Blinded"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Blinded"
    email         : "Blinded"
  - name          : "Blinded"
    affiliation   : "2"
  - name          : "Blinded"
    affiliation   : "1"
  - name          : "Blinded"
    affiliation   : "1"
affiliation:
  - id            : "1"
    institution   : "Blinded"
  - id            : "2"
    institution   : "Blinded"
author_note: >
  All procedures performed in studies involving human participants were approved by institutional research ethics committee and conducted in accordance with the Code of Professional Ethics of the Psychological Society of Ireland, and with the 1964 Helsinki declaration and its later amendments or comparable ethical standards. Informed consent was obtained from all individual participants included in the study. The authors declare that there are no potential conflicts of interest with respect to the research, authorship, and/or publication of this article. All authors consented to the submission of this manuscript.
abstract: >
  Six studies etc.
keywords          : "keywords"
wordcount         : "TBC"
bibliography: "../resources/bib/My Library.bib"
csl: "../resources/bib/apa6.csl"
figsintext        : yes
floatsintext      : yes
figurelist        : yes
tablelist         : yes
footnotelist      : no
lineno            : no
toc               : false
lang              : "en-US"
documentclass     : "apa7"
output:
  papaja::apa6_pdf
header-includes:
- \raggedbottom
editor_options: 
  chunk_output_type: console
---



```{r pilot1setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = FALSE)
# knitr::opts_chunk$set(eval = TRUE, echo = TRUE)
#knitr::opts_chunk$set(include = FALSE)
```



```{r pilot1load_libraries_cogload}
rm(list = ls())
#devtools::install_github("crsh/citr")
library(citr)
#install.packages("sjstats")
library(plyr)

library(foreign)
library(car)
#devtools::install_github("cillianmiltown/R_desnum")
library(desnum)
# install.packages(
#    "ggplot2",
#    repos = c("http://rstudio.org/_packages",
#    "http://cran.rstudio.com")
# )
# install.packages("ggplot2")
library("ggplot2")

#install.packages("vctrs")

library(extrafont)
#devtools::install_github("crsh/papaja")
#install.packages("afex")
library(papaja)
#library("dplyr")
library("afex")
library("tibble")
library(scales)
# if (!require("BiocManager", quietly = TRUE))
#     install.packages("BiocManager")
# 
# BiocManager::install("multtest")

#install.packages("multtest")
#install.packages("metap")
library(metap)
#install.packages("pwr")
library(pwr)
library(lsr)
#install.packages("sjstats")
library(sjstats)
library(DescTools)
#inatall.packages("ggstatsplot")
#library(ggstatsplot)
library(VGAM)
library(nnet)
library(mlogit)
library(reshape2)
#install.packages("powerMediation")
library("powerMediation")
library("ggpubr")


#source("load_all_data.R")

#devtools::install_github("benmarwick/wordcountaddin")
#library(wordcountaddin)
#wordcountaddin::text_stats("cogload_1to5_25Sept19.Rmd")
#setwd("..")
#setwd("manuscript_prep")
getwd()
```


```{r pilot1LoadData}
#source("~/Dropbox/College/research/Research_general/cog_load/moral_dumbfounding_and_cognitive_load/read_and_sort_raw_data.R")
#source("~/Dropbox/College/research/Research_general/cog_load/moral_dumbfounding_and_cognitive_load/load_study6_data.R")
rm(list = ls())

df3 <- read.csv("../data/pilot_data_long.csv")

x <- read.csv("../data/pilot_data_long_clean.csv")

MPS <- x %>% 
  select(MP_R1,MP_R2,MP_R3,MP_R4)

alpha1 <- ltm::cronbach.alpha(MPS)

```

# Pilot Study 1
The aim of this pilot study was to develop and test materials that could be used to study the dilution effect for moral characters. We developed diagnostic and non-diagnostic character descriptions. We hypothesized that moral evaluations of the diagnostic descriptions would be more severe (more immoral) than for the non-diagnostic descriptions.

## Pilot Study 1: Method
### Pilot 1: Participants and design
The pilot study was a within-subjects design. The independent variable was description type with two levels, *diagnostic* and *non-diagnostic*. We used two dependent variables. The first dependent variable was the four item moral perception scale (MPS-4), participants rated the characters on four dimensions using 7-point bipolar scales. The dimensions and scale endpoints were: Bad-Good, Immoral-Moral, Violent-Peaceful, Merciless-Empathetic, this showed excellent reliability, $\alpha$ = `r round(alpha1$alpha,digits=2)`. The second dependent variable was a single item moral perception measure (MM-1) which consisted of a 100-point slider ranging from 0 = *Very Immoral* to 100 = *Very Moral*. Both dependent variables were taken from Walker et al. [-@walker_better_2021].

A total sample of `r length(df3$gender)/6` (`r sum(df3$gender=="Female",na.rm=T)/6` female, `r sum(df3$gender=="Male", na.rm=T)/6` male, `r sum(df3$gender=="Non-binary / third gender", na.rm=T)/6` non-binary, `r sum(df3$gender=="Prefer not to say", na.rm=T)/6` prefer not to say; *M*~age~ = `r round(mean(df3$age, na.rm=T),digits=2)`, min = `r min(df3$age, na.rm=T)`, max = `r max(df3$age, na.rm=T)`, *SD* = `r round(sd(df3$age, na.rm=T),digits=2)`) started the survey.  Participants were recruited from MTurk.

```{r}
df1 <- read.csv("../data/pilot_data_long.csv")
df3 <- read.csv("../data/pilot_data_long_clean.csv")
table(df3$gender)
att_both <- (length(df1$gender)/6)-(length(df3$gender)/6)
```


We removed participants who failed both manipulation checks (*n* = `r att_both`), leaving a total sample of `r length(df3$gender)/6` participants (`r sum(df3$gender=="Female",na.rm=T)/6` female, `r sum(df3$gender=="Male", na.rm=T)/6` male, `r sum(df3$gender=="Non-binary / third gender", na.rm=T)/6` non-binary, `r sum(df3$gender=="Prefer not to say", na.rm=T)/6` prefer not to say; *M*~age~ = `r round(mean(df3$age, na.rm=T),digits=2)`, min = `r min(df3$age, na.rm=T)`, max = `r max(df3$age, na.rm=T)`, *SD* = `r round(sd(df3$age, na.rm=T),digits=2)`).

### Pilot 1: Procedure and materials
Data were collected using an online questionnaire presented with Qualtrics (www.qualtrics.com).  Participants were presented with descriptions of six characters.

Moral character descriptions were developed by combining descriptions relating to three different moral foundations. These descriptions were adapted from the items of the extended character morality questionnaire [@grizzard_validating_2020], and read as follows: (i) *Imagine a person named Sam. Throughout their life they have been known to be cruel, act unfairly, and to betray their own group*; (ii) *Imagine a person named Robin. Throughout their life they have been known to physically hurt others, treat some people differently to others, and show lack of loyalty*; (iii) *Imagine a person named Francis. Throughout their life they have been known to violate the standards of purity and decency, show lack of respect for authority, and treat people unequally* (iv) *Imagine a person named Alex. Throughout their life they have been known to cause others to suffer emotionally, to deny others their rights, and to cause chaos or disorder*.

We developed neutral descriptions that included information relating to physical appearance/attributes, hobbies/activities, and family information that read as follows: (i) *Imagine a person named Jackie. They have red hair, play tennis four times a month, and have one older sibling and one younger sibling*; (ii) *Imagine a person named Charlie. They are left-handed, drink tea in the morning, and have two older siblings and one younger sibling*.

Character descriptions did not specify the gender of the charcters, and all characters had names that could be either male or female (Sam, Robin, Francis, Alex, Jackie, Charlie). All participants read six descriptions, four moral descriptions and two neutral. Pilot Study 1 was pre-registered at \color{blue}[https://aspredicted.org/3VK_8FD](https://aspredicted.org/3VK_8FD)\color{black}.

## Pilot 1: Results

```{r}
x <- df3
sam <- x[which(x$scenario_abb=="Sam"),]
francis <- x[which(x$scenario_abb=="Francis"),]
alex <- x[which(x$scenario_abb=="Alex"),]
robin <- x[which(x$scenario_abb=="Robin"),]
jackie <- x[which(x$scenario_abb=="Jackie"),]
charlie <- x[which(x$scenario_abb=="Charlie"),]

```

```{r}
x <- df3
bad <- x[which(x$condition=="Diagnostic"),]
good <- x[which(x$condition=="Non-Diagnostic"),]
#good$scenario_abb <- droplevels(good$scenario_abb)

x <- bad
aov_full <- rstatix::anova_test(
  data=x
  , dv=R_tot
  , wid = ResponseId
  , within = scenario_abb)
aov1 <- rstatix::get_anova_table(aov_full)
aov1
aov1$DFd

pwc <- x %>%
  rstatix::pairwise_t_test(
    R_tot ~ scenario_abb, paired = TRUE,
    p.adjust.method = "bonferroni"
    )
pwc


d1 <- lsr::cohensD(R_tot~scenario_abb,good,method="paired")
t.test(R_tot~scenario_abb,good,paired=TRUE)
t1 <- t.test(good$R_tot~good$scenario_abb,paired=TRUE)
d1

```


The means and standard deviations for MPS-4 for each scenario are as follows: 
*Sam* (diagnostic),
*M*~MPS-4~ = `r mean(sam$R_tot)`, *SD*~MPS-4~ = `r sd(sam$R_tot)`,
*Francis* (diagnostic),
*M*~MPS-4~ = `r mean(francis$R_tot)`, *SD*~MPS-4~ = `r sd(francis$R_tot)`,
*Alex* (diagnostic),
*M*~MPS-4~ = `r mean(alex$R_tot)`, *SD*~MPS-4~ = `r sd(alex$R_tot)`,
*Robin* (diagnostic),
*M*~MPS-4~ = `r mean(robin$R_tot)`, *SD*~MPS-4~ = `r sd(robin$R_tot)`,
*Jackie* (non-diagnostic),
*M*~MPS-4~ = `r mean(jackie$R_tot)`, *SD*~MPS-4~ = `r sd(jackie$R_tot)`,
*Charlie* (non-diagnostic),
*M*~MPS-4~ = `r mean(charlie$R_tot)`, *SD*~MPS-4~ = `r sd(charlie$R_tot)`. For the diagnostic descriptions, there was no significant variation depending on the description, *F*(`r round(aov1$DFn)`,`r round(aov1$DFd)`) = `r aov1$F`, *p* `r paste(p_report(aov1$p))`, partial $\eta$^2^ = `r aov1$ges`. For the non-diagnostic descriptions there was no significant difference in ratings depending on description, *t*(`r t1$parameter`) = `r t1$statistic`, *p* `r paste(p_report(t1$p.value))`, *d* = `r d1`.


```{r}
x <- df3
bad <- x[which(x$condition=="Diagnostic"),]
good <- x[which(x$condition=="Non-Diagnostic"),]
#good$scenario_abb <- droplevels(good$scenario_abb)

x <- bad
aov_full <- rstatix::anova_test(
  data=x
  , dv=M1
  , wid = ResponseId
  , within = scenario_abb)
aov1 <- rstatix::get_anova_table(aov_full)
aov1
aov1$DFd

pwc <- x %>%
  rstatix::pairwise_t_test(
    M1 ~ scenario_abb, paired = TRUE,
    p.adjust.method = "bonferroni"
    )
pwc$p


d1 <- lsr::cohensD(M1~scenario_abb,good,method="paired")
t.test(M1~scenario_abb,good,paired=TRUE)
t1 <- t.test(good$M1~good$scenario_abb,paired=TRUE)
d1

```

The means and standard deviations for MM-1 for each scenario are as follows: 
*Sam* (diagnostic),
*M*~MM-1~ = `r mean(sam$M1)`, *SD*~MM-1~ = `r sd(sam$M1)`;
*Francis* (diagnostic),
*M*~MM-1~ = `r mean(francis$M1)`, *SD*~MM-1~ = `r sd(francis$M1)`;
*Alex* (diagnostic),
*M*~MM-1~ = `r mean(alex$M1)`, *SD*~MM-1~ = `r sd(alex$M1)`;
*Robin* (diagnostic),
*M*~MM-1~ = `r mean(robin$M1)`, *SD*~MM-1~ = `r sd(robin$M1)`;
*Jackie* (non-diagnostic),
*M*~MM-1~ = `r mean(jackie$M1)`, *SD*~MM-1~ = `r sd(jackie$M1)`;
*Charlie* (non-diagnostic),
*M*~MM-1~ = `r mean(charlie$M1)`, *SD*~MM-1~ = `r sd(charlie$M1)`. For the diagnostic descriptions, we observed significant variation depending on the description, *F*(`r round(aov1$DFn)`,`r round(aov1$DFd)`) = `r aov1$F`, *p* `r paste(p_report(aov1$p))`, partial $\eta$^2^ = `r paste(aov1$ges)`. When correcting for multiple comparisons, pairwise comparisons did not reveal significant differences between descriptions. We note that without correction, *Francis* appeared to be rated as more moral than both *Robin* (*p* `r paste(p_report(pwc$p[4]))`), and *Sam* (*p* `r paste(p_report(pwc$p[5]))`). For the non-diagnostic descriptions there was no significant difference in ratings depending on description, *t*(`r t1$parameter`) = `r t1$statistic`, *p* `r paste(p_report(t1$p.value))`, *d* = `r d1`.

```{r}
x <- df3
model0 <- lmerTest::lmer(R_tot ~ 1
                #   condition
                 + (1|ResponseId)
                # + (1|ResponseId:condition)
                , data = x
          #      , contrasts = list(condition = contr.sum  , valence = contr.sum)
            )

summary(model0)
model1 <- lmerTest::lmer(R_tot ~
                  condition
                + (1|ResponseId)
                + (1|ResponseId:condition)
                , data = x
                , contrasts = list(condition = contr.sum  )#, valence = contr.sum)
            )
summary(model1)
results_anova <- as.data.frame(anova(model0,model1))
results_anova
results_anova$Chisq[2]
results_anova$Df
p1 <- results_anova$`Pr(>Chisq)`[2]
p_report(p1)
#p_report(results_anova$`Pr(>Chisq)`)[2]
results_anova$AIC
summary_model1 <- summary(model1)


results_coef <- as.data.frame(summary_model1$coefficients)

results_coef$Estimate[2]
results_coef$`Std. Error`[2]
results_coef$df[2]
t1 <- results_coef$`t value`[2]
p2 <- results_coef$`Pr(>|t|)`[2]

#QuantPsyc::lm.beta(results_coef)
anova(model1)
```

We conducted a linear-mixed-effects model to test if condition influenced MPS-4 responses. Our outcome measure was MPS-4, our predictor variable was condition; we allowed intercepts and the effect of condition to vary across participants.
Overall, the model significantly predicted participants responses, and provided a better fit for the data than the baseline model, $\chi$^2^(`r results_anova$Df[2]`) = `r results_anova$Chisq[2]`, *p* `r paste(p_report(p1))`. Condition was a significant predictor in the model $b$ = `r results_coef$Estimate[2]`, *t*(`r results_coef$df[2]`) = `r t1`, *p* `r paste(p_report(p2))`, with the non-diagnostic descriptions being rated as more moral than the diagnostic descriptions of immoral characters Figure\ \@ref(fig:pilot1bothconditionplot).

```{r pilot1Rtotconditionplot, fig.cap="Pilot Study 1: Differences in MPS-4 depending on condition", include=FALSE}

mps4plot <- ggplot(x,aes(x=condition,y=R_tot))+
  geom_violin() +
  stat_summary(fun=mean, geom="point", shape=23, size=2)+
  geom_boxplot(width=0.1)+
#  geom_dotplot(binaxis='y', stackdir='center', dotsize=.05)+
# violin plot with jittered points
# 0.2 : degree of jitter in x direction
  geom_jitter(shape=16
              , position=position_jitter(0.15)
              , size=.3
              , color="dark grey") +
  xlab("Condition") +
  ylab("Moral Perception Scale (MPS-4)") +
  theme_bw() +
  theme(panel.border = element_blank(),
        axis.line = element_line(size = .2),
        strip.background  = element_blank(),
        panel.grid = element_blank(),
        plot.title=element_text(#family="Times",
                                size=12
                                ),
        legend.text=element_text(#family="Times",
                                 size=8
                                 ),
          legend.title=element_text(#family="Times",
                                    size=10
                                    ),
          axis.text=element_text(#family="Times",
                                 colour = "black",
                                 size=8
                                 ),
          axis.ticks.x = element_blank(),
          axis.title=element_text(#family="Times",
                                  size=12
                                  ),
          strip.text=element_text(#family = "Times",
                                  size = 12
                                  ),
         # strip.background = element_rect(fill = "white"),
          legend.position="right")




```


```{r pilot1M1conditionplot, fig.cap="Pilot Study 1: Differences in MM1 depending on condition", include=FALSE}

M1plot <- ggplot(x,aes(x=condition,y=M1))+
  geom_violin() +
  stat_summary(fun=mean, geom="point", shape=23, size=2)+
  geom_boxplot(width=0.1)+
#  geom_dotplot(binaxis='y', stackdir='center', dotsize=.05)+
# violin plot with jittered points
# 0.2 : degree of jitter in x direction
  geom_jitter(shape=16
              , position=position_jitter(0.15)
              , size=.3
              , color="dark grey") +
  xlab("Condition") +
  ylab("Moral Perception Measure (MM-1)")+
  theme_bw() +
  theme(panel.border = element_blank(),
        axis.line = element_line(size = .2),
        strip.background  = element_blank(),
        panel.grid = element_blank(),
        plot.title=element_text(#family="Times",
                                size=12
                                ),
        legend.text=element_text(#family="Times",
                                 size=8
                                 ),
          legend.title=element_text(#family="Times",
                                    size=10
                                    ),
          axis.text=element_text(#family="Times",
                                 colour = "black",
                                 size=8
                                 ),
          axis.ticks.x = element_blank(),
          axis.title=element_text(#family="Times",
                                  size=12
                                  ),
          strip.text=element_text(#family = "Times",
                                  size = 12
                                  ),
         # strip.background = element_rect(fill = "white"),
          legend.position="right")




```


```{r}
figure <- ggarrange(mps4plot
                    , M1plot
                    #,labels = c("A")
                    , ncol = 2
                    , nrow = 1
                    , widths = c(0.485, 0.5))
figure
```


```{r pilot1bothconditionplot, fig.cap="Pilot Study 1: Differences in moral perception depending on condition", out.width = "\\textwidth", fig.pos = "!h", include=TRUE}

suppressWarnings(print(figure))

```



```{r}
model0 <- lmerTest::lmer(M1 ~ 1
                #   condition
                 + (1|ResponseId)
                # + (1|ResponseId:condition)
                , data = x
            #    , contrasts = list(condition = contr.sum  , valence = contr.sum)
            )

summary(model0)
model1 <- lmerTest::lmer(M1 ~
                  condition
                + (1|ResponseId)
                + (1|ResponseId:condition)
                , data = x
                , contrasts = list(condition = contr.sum  )#, valence = contr.sum)
            )

results_anova <- as.data.frame(anova(model0,model1))
results_anova
results_anova$Chisq[2]
results_anova$Df
p1 <- results_anova$`Pr(>Chisq)`[2]
p_report(p1)
#p_report(results_anova$`Pr(>Chisq)`)[2]
results_anova$AIC
summary_model1 <- summary(model1)


results_coef <- as.data.frame(summary_model1$coefficients)

results_coef$Estimate[2]
results_coef$`Std. Error`[2]
results_coef$df[2]
results_coef$`t value`[2]
t1 <- results_coef$`t value`[2]

p2 <- results_coef$`Pr(>|t|)`[2]

#QuantPsyc::lm.beta(results_coef)
anova(model1)
```


We conducted a linear-mixed-effects model to test if condition influenced MM-1 responses. Our outcome measure was MM-1, our predictor variable was condition; we allowed intercepts and the effect of condition to vary across participants. Overall, the model significantly predicted participants responses, and provided a better fit for the data than the baseline model, $\chi$^2^(`r results_anova$Df[2]`) = `r results_anova$Chisq[2]`, *p* `r paste(p_report(p1))`. Condition was a significant predictor in the model $b$ = `r results_coef$Estimate[2]`, *t*(`r results_coef$df[2]`) = `r t1`, *p* `r paste(p_report(p2))`, with the non-diagnostic descriptions being rated as more moral than the diagnostic descriptions, see Figure\ \@ref(fig:pilot1bothconditionplot).


