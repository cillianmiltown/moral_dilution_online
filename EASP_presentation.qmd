---
title: "The Moral Dilution Effect"
authors: "Cillian McHugh & Eric R. Igou"
institute: "Department of Psychology Seminar Series"
bibliography: "resources/bib/My Library.bib"
csl: resources/bib/apa.csl
format:
  revealjs:
    slide-number: false
    logo: "resources/images/ul-psychology-logos.png"
    footer: "Cillian McHugh - 2/7/23"
    theme: mytheme.scss
    chalkboard: 
      buttons: true
editor: source
---

```{r}
library(desnum)
library(tidyverse)
```


## Quarto

```{r}
#| include: false

rm(list = ls())


df1 <- read.csv("data/study5_data_long.csv")
df3 <- read.csv("data/study5_data_long_clean.csv")
df_long_clean <- df3
table(df3$gender)
length(df1$gender)/6
length(levels(as.factor(df1$ResponseId)))-length(levels(as.factor(df3$ResponseId)))
length(df3$gender)/4

att_both <- length(levels(as.factor(df1$ResponseId)))-length(levels(as.factor(df3$ResponseId)))



df_long_clean <- df3

x <- df3
x$attn_chk_1Q
x$attn_chk_2_Q
x <- x[which(x$attn_chk_2_Q==2|x$attn_chk_2_Q==5),]
x <- x[which(x$attn_chk_1Q==7),]
df_long_extra_clean <- x

x <- df3
# 
# df3 <- x
# att_both <- length(levels(as.factor(df1$ResponseId)))-length(levels(as.factor(df3$ResponseId)))


good <- x[which(x$valence=="good"),]
bad <- x[which(x$valence=="bad"),]

good$R_tot_recoded <- 7 - good$R_tot
bad$R_tot_recoded <- bad$R_tot

good$M1_recoded <- 100 - good$M1
bad$M1_recoded <- bad$M1

df_recoded <- rbind(good,bad)


x <- df_recoded # df_long_extra_clean

good <- x[which(x$valence=="good"),]
bad <- x[which(x$valence=="bad"),]

good$R_tot_recoded <- 7 - good$R_tot
bad$R_tot_recoded <- bad$R_tot

good$M1_recoded <- 100 - good$M1
bad$M1_recoded <- bad$M1

df_recoded_extra_clean <- rbind(good,bad)

x <- df_recoded_extra_clean

```


```{r}
#| include: false
x <- df3
sam <- x[which(x$scenario=="sam"),]
francis <- x[which(x$scenario=="francis"),]
alex <- x[which(x$scenario=="alex"),]
robin <- x[which(x$scenario=="robin"),]

```

```{r}
#| include: false
x <- df3
tapply(x$R_tot, x$scenario, descriptives)
# bad <- x[which(x$condition=="diagnostic"),]
# good <- x[which(x$condition=="non-diagnostic"),]
#good$scenario_abb <- droplevels(good$scenario_abb)
x$valence
#x <- bad
aov_full <- rstatix::anova_test(
  data=x
  , dv=R_tot
  , wid = ResponseId
  , within = scenario)
aov1 <- rstatix::get_anova_table(aov_full)
aov1
aov1$DFd

pwc <- x %>%
  rstatix::pairwise_t_test(
    R_tot ~ scenario, paired = TRUE,
    p.adjust.method = "bonferroni"
    )
pwc
p_report(pwc$p[2])

# d1 <- lsr::cohensD(R_tot~condition,x,method="paired")
# t.test(R_tot~condition,x,paired=TRUE)
# t1 <- t.test(x$R_tot~x$scenario,paired=TRUE)
# d1
lapply(pwc$p.adj,p_report)
```


## Descriptives


The means and standard deviations for MPS-4 for each scenario are as follows: 
*Sam* (good),
*M*~MPS-4~ = `r mean(sam$R_tot)`, *SD*~MPS-4~ = `r sd(sam$R_tot)`,
*Francis* (bad),
*M*~MPS-4~ = `r mean(francis$R_tot)`, *SD*~MPS-4~ = `r sd(francis$R_tot)`,
*Alex* (bad),
*M*~MPS-4~ = `r mean(alex$R_tot)`, *SD*~MPS-4~ = `r sd(alex$R_tot)`,
*Robin* (good),
*M*~MPS-4~ = `r mean(robin$R_tot)`, *SD*~MPS-4~ = `r sd(robin$R_tot)`. There was significant variation depending on the description, *F*(`r round(aov1$DFn)`,`r round(aov1$DFd)`) = `r aov1$F`, *p* `r paste(p_report(aov1$p))`, partial $\eta$^2^ = `r aov1$ges`. Both the *good* characters (*Robin* and *Sam*) were rated significantly more favorably than both the *bad* characters (*Alex* and *Francis*; all *p*s < .001). There were no differences between *Robin* and *Sam* (*good*: *p* `r paste(p_report(pwc$p[6]))`) or between *Alex* and *Francis* (*bad*; *p* `r paste(p_report(pwc$p[1]))`). 



```{r}
#| include: false


x <- df3
# bad <- x[which(x$condition=="diagnostic"),]
# good <- x[which(x$condition=="non-diagnostic"),]
#good$scenario_abb <- droplevels(good$scenario_abb)

#x <- bad
aov_full <- rstatix::anova_test(
  data=x
  , dv=M1
  , wid = ResponseId
  , within = scenario)
aov1 <- rstatix::get_anova_table(aov_full)
aov1
aov1$DFd

pwc <- x %>%
  rstatix::pairwise_t_test(
    M1 ~ scenario, paired = TRUE,
    p.adjust.method = "bonferroni"
    )
pwc
p_report(pwc$p[2])
lapply(pwc$p.adj, p_report)
paste(pwc$p.adj[2])
pwc$p[2]
# d1 <- lsr::cohensD(M1~condition,x,method="paired")
# t.test(M1~scenario,x,paired=TRUE)
# t1 <- t.test(x$M1~x$scenaro,paired=TRUE)
# d1

```

## Descriptives

The means and standard deviations for MM-1 for each scenario are as follows: 
*Sam* (good),
*M*~MM-1~ = `r mean(sam$M1)`, *SD*~MM-1~ = `r sd(sam$M1)`;
*Francis* (bad),
*M*~MM-1~ = `r mean(francis$M1)`, *SD*~MM-1~ = `r sd(francis$M1)`;
*Alex* (bad),
*M*~MM-1~ = `r mean(alex$M1)`, *SD*~MM-1~ = `r sd(alex$M1)`;
*Robin* (good),
*M*~MM-1~ = `r mean(robin$M1)`, *SD*~MM-1~ = `r sd(robin$M1)`. There was significant variation depending on the description, *F*(`r round(aov1$DFn)`,`r round(aov1$DFd)`) = `r aov1$F`, *p* `r paste(p_report(aov1$p))`, partial $\eta$^2^ = `r paste(aov1$ges)`. Again, the *good* characters (*Robin* and *Sam*) were rated significantly more favorably than the *bad* characters (*Alex* and *Francis*; all *p*s < .001). There were no differences between *Robin* and *Sam* (*good*: *p* `r paste(p_report(pwc$p[6]))`) or between *Alex* and *Francis* (*bad*; *p* `r paste(p_report(pwc$p[1]))`). 



```{r}
#| include: false
x <- df3
x <- df_recoded

#x <- df_recoded_extra_clean


model0 <- lmerTest::lmer(R_tot_recoded ~ 1
                #   condition
                 + (1|ResponseId)
                # + (1|ResponseId:condition)
                , data = x
          #      , contrasts = list(condition = contr.sum  , valence = contr.sum)
            )

summary(model0)
model1 <- lmerTest::lmer(R_tot ~
                  condition*scenario
                + (1|ResponseId)
                + (1|ResponseId:condition)
                , data = x
                , contrasts = list(condition = contr.sum  )#, valence = contr.sum)
            )



model1 <- lmerTest::lmer(R_tot_recoded ~
                  condition*valence
                + (1|ResponseId)
                + (1|ResponseId:condition)
                + (1|ResponseId:valence)
                , data = x
                , contrasts = list(condition = contr.sum, valence = contr.sum )#, valence = contr.sum)
            )
summary(model1)
anova(model1)
results_anova <- as.data.frame(anova(model0,model1))
results_anova
results_anova$Chisq[2]
results_anova$Df
p1 <- results_anova$`Pr(>Chisq)`[2]
p_report(p1)
#p_report(results_anova$`Pr(>Chisq)`)[2]
results_anova$AIC
summary_model1 <- summary(model1)
summary_model1
results_coef <- as.data.frame(summary_model1$coefficients)
results_coef

aov1 <- anova(model1)
f3a <- aov1$`F value`[1]
f3b <- aov1$`F value`[2]
f3c <- aov1$`F value`[3]
p3a <- aov1$`Pr(>F)`[1]
p3b <- aov1$`Pr(>F)`[2]
p3c <- aov1$`Pr(>F)`[3]

results_coef$Estimate[2]
results_coef$`Std. Error`[2]
results_coef$df[2]
t1 <- results_coef$`t value`[2]
p2 <- results_coef$`Pr(>|t|)`[2]

#QuantPsyc::lm.beta(results_coef)
anova(model1)
```

## Test

We conducted a linear-mixed-effects model to test if our predictors influenced MPS-4 responses. Our outcome measure was MPS-4, our predictor variables were condition and valence; we allowed intercepts and the effects of condition and valence to vary across participants.
Overall, the model significantly predicted participants responses, and provided a better fit for the data than the baseline model,
$\chi$^2^(`r results_anova$Df[2]`) = `r results_anova$Chisq[2]`, *p* `r paste(p_report(p1))`. 
Overall, there was a significant main effect for condition,
*F*(`r aov1$NumDF[1]`, `r round(aov1$DenDF[1])`) = `r f3a`, *p* `r paste(p_report(p3a))`;
valence significantly predicted responses,
*F*(`r aov1$NumDF[2]`, `r round(aov1$DenDF[2])`) = `r f3b`, *p* `r paste(p_report(p3b))`;
and there was no significant condition $\times$ valence interaction,
*F*(`r aov1$NumDF[3]`, `r round(aov1$DenDF[3])`) = `r f3c`, *p* `r paste(p_report(p3c))`.



```{r}
#| include: false
x <- df3
x <- df_recoded
model0 <- lmerTest::lmer(M1 ~ 1
                #   condition
                 + (1|ResponseId)
                # + (1|ResponseId:condition)
                , data = x
          #      , contrasts = list(condition = contr.sum  , valence = contr.sum)
            )

summary(model0)
model1 <- lmerTest::lmer(M1 ~
                  condition*scenario
                + (1|ResponseId)
                + (1|ResponseId:condition)
                , data = x
                , contrasts = list(condition = contr.sum  )#, valence = contr.sum)
            )



model1 <- lmerTest::lmer(M1_recoded ~
                  condition*valence
                + (1|ResponseId)
                + (1|ResponseId:condition)
                + (1|ResponseId:valence)
                , data = x
                , contrasts = list(condition = contr.sum, valence = contr.sum )#, valence = contr.sum)
            )
summary(model1)
anova(model1)
results_anova <- as.data.frame(anova(model0,model1))
results_anova
results_anova$Chisq[2]
results_anova$Df
p1 <- results_anova$`Pr(>Chisq)`[2]
p_report(p1)
#p_report(results_anova$`Pr(>Chisq)`)[2]
results_anova$AIC
summary_model1 <- summary(model1)
summary_model1
results_coef <- as.data.frame(summary_model1$coefficients)
results_coef

aov1 <- anova(model1)
f3a <- aov1$`F value`[1]
f3b <- aov1$`F value`[2]
f3c <- aov1$`F value`[3]
p3a <- aov1$`Pr(>F)`[1]
p3b <- aov1$`Pr(>F)`[2]
p3c <- aov1$`Pr(>F)`[3]

results_coef$Estimate[2]
results_coef$`Std. Error`[2]
results_coef$df[2]
t1 <- results_coef$`t value`[2]
p2 <- results_coef$`Pr(>|t|)`[2]

#QuantPsyc::lm.beta(results_coef)
anova(model1)
```

## Test

We conducted a linear-mixed-effects model to test if our predictors influenced MM-1 responses. The model was the same as the previous model, with a change to the outcome measure, our outcome measure for this model was MM-1. As above, our predictor variables were condition and valence; we allowed intercepts and the effects of condition and valence to vary across participants.
Overall, the model significantly predicted participants responses, and provided a better fit for the data than the baseline model,
$\chi$^2^(`r results_anova$Df[2]`) = `r results_anova$Chisq[2]`, *p* `r paste(p_report(p1))`. 
Overall there was a main effect for condition,
*F*(`r aov1$NumDF[1]`, `r round(aov1$DenDF[1])`) = `r f3a`, *p* `r paste(p_report(p3a))`;
valence significantly predicted responses,
*F*(`r aov1$NumDF[2]`, `r round(aov1$DenDF[2])`) = `r f3b`, *p* `r paste(p_report(p3b))`;
and there was no significant condition $\times$ valence interaction,
*F*(`r aov1$NumDF[3]`, `r round(aov1$DenDF[3])`) = `r f3c`, *p* `r paste(p_report(p3c))`.

For both MP-4 and MM-1 we found a main effect for condition and valence, and there was no condition $\times$ valence interaction. We conducted follow-up analyses to test the if the main effect for condition holds for both good and bad descriptions separately.


## Bullets

When you click the **Render** button a document will be generated that includes:

-   Content authored with markdown
-   Output from executable code

## Code

When you click the **Render** button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:

```{r}
1 + 1
```


